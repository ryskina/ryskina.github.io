@article{hupkes2022taxonomy,
  abbr={Nat Mach Intell},
  title={A taxonomy and review of generalization research in NLP},
  author = {Dieuwke Hupkes and Mario Giulianelli and Verna Dankers and Mikel Artetxe and
          Yanai Elazar and Tiago Pimentel and Christos Christodoulopoulos and Karim Lasri and
          Naomi Saphra and Arabella Sinclair and Dennis Ulmer and Florian Schottmann and
          Khuyagbaatar Batsuren and Kaiser Sun and Koustuv Sinha and Leila Khalatbari and
          Maria Ryskina and Rita Frieske and Ryan Cotterell and Zhijing Jin},
  year={2023},
  journal = {Nature Machine Intelligence},
  arxiv="2210.03050",
  doi="10.1038/s42256-023-00729-y",
  html="https://www.nature.com/articles/s42256-023-00729-y",
  abstract="The ability to generalize well is one of the primary desiderata for models of natural language processing (NLP), but what ‘good generalization’ entails and how it should be evaluated is not well understood. In this Analysis we present a taxonomy for characterizing and understanding generalization research in NLP. The proposed taxonomy is based on an extensive literature review and contains five axes along which generalization studies can differ: their main motivation, the type of generalization they aim to solve, the type of data shift they consider, the source by which this data shift originated, and the locus of the shift within the NLP modelling pipeline. We use our taxonomy to classify over 700 experiments, and we use the results to present an in-depth analysis that maps out the current state of generalization research in NLP and make recommendations for which areas deserve attention in the future.",
  website={https://genbench.org/},
  display_authors = 3,
}

@article{ivanova2025elements,
    abbr={TACL},
    author    = {Ivanova, Anna A. and Sathe, Aalok and Lipkin, Benjamin and Kumar, Unnathi and Radkani,
                Setayesh and Clark, Thomas H. and Kauf, Carina and Hu, Jennifer and Pramod, R.~T. and
                Grand, Gabriel and Paulun, Vivian C. and Ryskina, Maria and Aky\"{u}rek, Ekin and Wilcox,
                Ethan and Rashid, Nafisa and Choshen, Leshem and Levy, Roger and Fedorenko, Evelina
                and Tenenbaum, Josh and Andreas, Jacob},
    title     = {Elements of World Knowledge (EWoK): A Cognition-Inspired Framework for Evaluating
                Basic World Knowledge in Language Models},
    journal   = {Accepted to Transactions of the Association for Computational Linguistics},
    year      = {2025},
    abstract="The ability to build and reason about models of the world is essential for situated language understanding. But evaluating world modeling capabilities in modern AI systems—especially those based on
              language models—has proven challenging, in large part because of the difficulty of disentangling conceptual knowledge about the world from knowledge of surface
              co-occurrence statistics. This paper presents Elements of World Knowledge (EWoK), a framework for evaluating language models' understanding of the
              conceptual knowledge underlying world modeling. EWoK targets specific concepts from multiple knowledge domains known
              to be important for world modeling in humans, from social interactions (help, deceive) to spatial relations (left, right).
              Objects, agents, and locations in the items can be flexibly filled in, enabling easy generation of multiple controlled datasets.
              We then introduce EWoK-CORE-1.0, a dataset of 4,374 items covering 11 world knowledge domains. We evaluate 20 open-weights large language models (1.3B–70B
              parameters) and compare them with human performance. All tested models perform worse than humans, with results varying drastically across domains. Performance
              on social interactions and social properties was highest and performance on physical relations and spatial relations was lowest.
              Overall, this dataset highlights simple cases where even large models struggle and presents rich avenues for targeted research
              on LLM world modeling capabilities.", 
    arxiv="2405.09605v1",
    website={https://ewok-core.github.io/},
    equal_first = 3,
}